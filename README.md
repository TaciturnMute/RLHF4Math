# Reinforcement Learning for Mathematical Reasoning (RLHF4Math)

[![Awesome](https://awesome.re/badge.svg)](https://github.com/TaciturnMute/RLHF4Math/tree/main) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

This repository is the reading list on ***Reinforcement Learning Human Feedback for Mathematical Reasoning (RLHF4Math)***. Related papers, web blogs, datasets, and classic books are included. 

**Contributors**: [yehangcheng]() @XXX, [Wenhao Yan](https://gitee.com/ByTheSea) @NEU, [Chen Bo]() @XXX

:bell: If you have any suggestions or notice something we missed, please don't hesitate to let us know. You can directly email Wenhao Yan (wenhao19990616@gmail.com), or post an issue on this repo.

## ü¶Å Papers

### Related Survey
<small>This section mainly covers surveys about RLHF for math, but also covers other areas.</small>

- **Reasoning with Language Model Prompting: A Survey**, [[paper](https://arxiv.org/pdf/2212.09597.pdf)]
- **Towards Reasoning in Large Language Models: A Survey**, [[paper](https://arxiv.org/pdf/2212.10403.pdf)]
- **A Survey of Deep Learning for Mathematical Reasoning**, [[paper](https://aclanthology.org/2023.acl-long.817.pdf)]
- **A Survey on In-context Learning**, [[paper](https://arxiv.org/pdf/2301.00234.pdf)]
- **A Survey on Transformers in Reinforcement Learning**, [[paper](https://arxiv.org/pdf/2301.03044.pdf)]

### Math Reasoning
<small>This section covers RLHF for math reasoning.</small>

- **Solving Math Word Problems via Cooperative Reasoning induced Language Models**, [[paper](https://aclanthology.org/2023.acl-long.245.pdf)]
- **Interpretable Math Word Problem Solution Generation via Step-by-step Planning**, [[paper](https://aclanthology.org/2023.acl-long.379.pdf)]
- **Compositional Mathematical Encoding for Math Word Problems**, [[paper](https://aclanthology.org/2023.findings-acl.635.pdf)]
- **Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models**, [[paper](https://aclanthology.org/2023.acl-long.147.pdf)]
- **GeoDRL: A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning**, [[paper](https://aclanthology.org/2023.findings-acl.850.pdf)]
- **A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models**, [[paper](https://aclanthology.org/2023.acl-long.32.pdf)]
- **Math Word Problem Solving by Generating Linguistic Variants of Problem Statements**, [[paper](https://aclanthology.org/2023.acl-srw.49.pdf)]
- **Making Language Models Better Reasoners with Step-Aware Verifier**, [[paper](https://aclanthology.org/2023.acl-long.291.pdf)]
- **ALERT: Adapting Language Models to Reasoning Tasks**, [[paper](https://aclanthology.org/2023.acl-long.60.pdf)]
- **Distilling Reasoning Capabilities into Smaller Language Models**, [[paper](https://aclanthology.org/2023.findings-acl.441.pdf)]
- **Preference Ranking Optimization for Human Alignment**, [[paper](https://arxiv.org/pdf/2306.17492.pdf)]
- **Direct Preference Optimization:Your Language Model is Secretly a Reward Model**, [[paper](https://arxiv.org/pdf/2305.18290.pdf)]
- **Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning**, [[paper](https://promptpg.github.io/)]
- **SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions**, [[paper](https://arxiv.org/pdf/2212.10560.pdf)]
- **OFFLINE REINFORCEMENT LEARNING WITH IMPLICIT Q-LEARNING**, [[paper](https://arxiv.org/pdf/2110.06169.pdf)]
- **OFFLINE RL FOR NATURAL LANGUAGE GENERATION WITH IMPLICIT LANGUAGE Q LEARNING**, [[paper](https://arxiv.org/pdf/2206.11871.pdf)]
- üíê **Chain of Thought Prompting Elicits Reasoning in Large Language Models**, [[paper](https://arxiv.org/pdf/2201.11903.pdf)]
- **LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS**, [[paper](https://arxiv.org/pdf/2205.10625.pdf)]
- üíê **Let‚Äôs Verify Step by Step**, [[paper](https://arxiv.org/abs/2305.20050)]
- **MathPrompter Mathematical Reasoning using Large Language Models**, [[paper](https://arxiv.org/pdf/2303.05398.pdf)]
- **Measuring Mathematical Problem Solving With the MATH Dataset**, [[paper](https://arxiv.org/pdf/2103.03874.pdf)]
- üíê **Solving math word problems with process and outcome-based feedback**, [[paper](https://arxiv.org/pdf/2211.14275.pdf)]
- üíê **STaR Self-Taught Reasoner Bootstrapping Reasoning With Reasoning**, [[paper](https://arxiv.org/pdf/2203.14465.pdf)]
- üíê **Training Verifiers to Solve Math Word Problems**, [[paper](https://arxiv.org/pdf/2110.14168.pdf)]

### RLHF suplement
<small>This section covers other research and general content for RLHF.</small>

- **CodeRL Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning**, [[paper](https://arxiv.org/pdf/2207.01780.pdf)]
- **RAFT Reward rAnked FineTuning for Generative Foundation Model Alignment**, [[paper](https://arxiv.org/pdf/2304.06767.pdf)]
- **IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION**, [[paper](https://arxiv.org/pdf/2210.01241.pdf)]
- **WizardCoder: Empowering Code Large Language Models with Evol-Instruct**, [[paper](https://arxiv.org/pdf/2306.08568.pdf)]
- **Constitutional AI: Harmlessness from AI Feedback**, [[paper](https://arxiv.org/pdf/2212.08073.pdf)]

### RL supplement
<small>This section covers RL prerequisite foundation for RLHF.</small>

- **Trust Region Policy Optimization**, [[paper](https://arxiv.org/pdf/1502.05477.pdf)]
- üíê **Proximal Policy Optimization Algorithms**, [[paper](https://arxiv.org/pdf/1707.06347.pdf)]
- **Mastering the Game of Go without Human Knowledge**, [[paper](https://faculty.washington.edu/jwilker/559/2018/go.pdf)]
- **Deep reinforcement learning from human preferences**, [[paper](https://arxiv.org/pdf/1706.03741.pdf)]
- **Thinking Fast and Slow with Deep Learning and Tree Search**, [[paper](https://arxiv.org/pdf/1705.08439.pdf)]

### NLP supplement
<small>This section covers NLP prerequisite foundation for RLHF.</small>

- **add title**, [[paper](add link)]

### LLM supplement
<small>This section covers LLM supplement content.</small>

- **Improving Language Understanding by Generative Pre-Training**, [[paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)]
- **Language Models are Unsupervised Multitask Learners**, [[paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)]
- **Language Models are Few-Shot Learners**, [[paper](https://arxiv.org/pdf/2005.14165.pdf)]
- **GPT-4 Technical Report**, [[paper](https://arxiv.org/pdf/2303.08774.pdf)]
- üíê **Training language models to follow instructions with human feedback**, [[paper](https://arxiv.org/pdf/2203.02155.pdf)]
- üíê **GLM: General Language Model Pretraining with Autoregressive Blank Infilling**, [[paper](https://arxiv.org/pdf/2103.10360.pdf)]
- üíê **GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL**, [[paper](https://arxiv.org/pdf/2210.02414.pdf)]


## üé∂ Blogs & Docs & Books

- üçë [blog] **Lil'Log**, [[RL,NLP,and so on](https://lilianweng.github.io/)]
- üçë [blog] **Yao Fu's Blog**, [[Open-source the knowledge](https://franxyao.github.io/)]
- [docs] **Stable-Baselines3 Docs**, [[Reliable Reinforcement Learning Implementations](https://stable-baselines3.readthedocs.io/en/master/)]
- [docs] **OpenAI Spinning Up**, [[An educational resource produced by OpenAI to learning RL easier](https://spinningup.openai.com/en/latest/)]
- [book] **Reinforcement Learning: An Introduction(Chinese Version)**, [[Classical Works in RL](https://rl.qiwihui.com/zh_CN/latest/index.html)]


## üêÆ Datasets

### Math

### Other
- **Anthropic Helpful and Harmless RLHF**, ***reward modeling*** [[git](https://github.com/anthropics/hh-rlhf)] [[paper](https://arxiv.org/pdf/2204.05862.pdf)]
- **OpenAI Summarize**, ***reward modeling*** [[git](https://github.com/openai/summarize-from-feedback)] [[paper](https://arxiv.org/pdf/2009.01325.pdf)]
- **OpenAI Web GPT**, ***reward modeling*** [[blog](https://openai.com/research/webgpt)] [[paper](https://arxiv.org/pdf/2112.09332.pdf)] [[hugging face](https://huggingface.co/datasets/openai/webgpt_comparisons)]
- **stack-exchange-preferences**, ***reward modeling*** [[hugging face](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)]
- **Stanford Human Preferences Dataset (SHP)**, ***reward modeling*** [[hugging face](https://huggingface.co/datasets/stanfordnlp/SHP)]
- **TruthfulQA**, ***evaluate truthful*** [[hugging face](https://huggingface.co/datasets/truthful_qa)]
- **ToxiGen**, ***evaluate toxicity*** [[hugging face](https://huggingface.co/datasets/skg/toxigen-data)]
- **Bias in Open-ended Language Generation Dataset (BOLD)**, ***evaluate fairness*** [[hugging face](https://huggingface.co/datasets/AlexaAI/bold)]




## üé® Code

- [WizardLM] **WizardLM: An Instruction-following LLM Using Evol-Instruct**, [[git](https://github.com/nlpxucan/WizardLM)]
- [Self-Instruct] **Self-Instruct: Aligning LM with Self Generated Instructions**, [[git](https://github.com/yizhongw/self-instruct)]
- [ConstitutionalHarmlessness] **Constitutional AI: Harmlessness from AI Feedback**, [[git](https://github.com/anthropics/constitutionalharmlessnesspaper)]
- [project-name] **project full name**, [[where](link)]



## Citation
If you find this repo useful, please kindly cite our survey:

```
@article{lu2022dl4math,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2212.10535},
  year={2022}
}
```